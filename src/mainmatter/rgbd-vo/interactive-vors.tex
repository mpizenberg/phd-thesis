\chapter{Interactive Visual Odometry on the Web}%
\label{cha:interactive_vo_on_the_web}

\minitoc%
\clearpage

\section{Visual Odometry in Rust (VORS)}%
\label{sec:vors}

\subsection{Overview of VORS}%
\label{sub:vors-overview}

VORS is a sparse, keyframe-based, RGB-D, direct visual odometry \Axel{algorithm?}.
As presented in Section~\ref{sub:appearance-based},
VORS belongs to the family of direct visual odometry,
based on the image alignment optimization problem.
Contrary to monocular visual odometry, which requires an estimation
of points depth, we focus on the RGB-D case,
where the necessary depth information for reprojection is provided by a sensor.
In order to reduce drift when the camera moves slowly,
image alignment is performed from a fixed keyframe instead of the previous frame.
Decision to change the keyframe is done heuristically;
details are presented in Section~\ref{sub:multi-res-direct-image-alignment}.
Finally, our algorithm uses a sparse subset of points in the image,
which has been shown~\cite{engel2017direct} to be sufficient to track the camera motion.
Disregarding sparsity and robustness, discussed later,
our algorithm is very similar to DVO~\cite{kerl2013robust},
and its predecessor~\cite{steinbrucker2011real}.
An overview of the full pipeline of VORS is represented in Figure~\ref{fig:overview-vors}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\linewidth]{assets/img/overview-vors.pdf}
	\caption{Overview of VORS pipeline.}%
	\label{fig:overview-vors}
\end{figure}

\subsection{Sparse Points Selection}%
\label{sub:sparse-points}

Engel et al.\ demonstrated in DSO~\cite{engel2017direct} that using a sparse subset
of points for the direct image alignment problem still results in precise camera locations
at the condition that points satisfy two conditions,
\begin{itemize}
\setlength\itemsep{-0.5em}
	\item they should be well distributed in the image,
	\item and they should be located at positions with higher gradients magnitudes.
\end{itemize}
Selected points are called candidate points,
and their selection mechanism is rather complex, based on at least ten parameters.
The core idea is to regularly divide an image in tiles.
One subdivision produces big tiles, called regions,
and another generates small tiles, called blocks.
One pixel is selected per block if its gradient is higher than a local threshold
depending on the region containing the pixel.
In practice, blocks are actually multi-scale, with three default levels.
If none of the four ``level $n$'' blocks of a ``level $(n+1)$'' block elected a candidate point,
the ``level $(n+1)$'' block checks whether a pixel satisfies a lower threshold.
The factor between block thresholds at different levels is one of the parameters.
Another mechanism aims at obtaining a target amount of candidate points.
That amount can be approximated from blocks base size but it might differ.
Depending on the difference with the target count,
the algorithm will choose between the three following options,
(1) keep candidates, (2) randomly select a sample of the candidates,
or (3) change the block base size and restart from scratch.

In contrast, our sparse selection approach is rather simple.
We embraced the idea that each level of the image pyramid should exhibit
the same property of well distributed points with higher density near higher gradients.
Intuitively, it should help convergence of the alignment problem to a global minimum.
We thus propose a simple coarse-to-fine selection mechanism.
At the lowest resolution, all points are candidates.
At each resolution step, one or two pixels of the four are elected candidates per
parent candidate pixel \Axel{pas tres clair} in the lower resolution.
A threshold based on pixel gradients is used to determine the which pixels should be selected.
This sparse selection scheme ensures that there is at least one candidate per region
corresponding to one pixel at the lowest resolution.
It also increases the density of candidates in higher gradients areas.
The level of the lowest resolution is a common parameter for candidates selection
and for the multi-resolution direct image alignement.
A visualization of this selection algorithm
is presented in Figure~\ref{fig:coarse-to-fine-candidates}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.23\linewidth]{assets/img/candidates-icl-4.png}
	\hfill
	\includegraphics[width=0.23\linewidth]{assets/img/candidates-icl-3.png}
	\hfill
	\includegraphics[width=0.23\linewidth]{assets/img/candidates-icl-2.png}
	\hfill
	\includegraphics[width=0.23\linewidth]{assets/img/candidates-icl-1.png}
	\caption{Coarse-to-fine candidate selection of VORS.
	Candidates are represented in red.
	All points are kept at the lowest resolution.
	Each higher resolution elects one or two candidates per parent candidate.}%
	\label{fig:coarse-to-fine-candidates}
\end{figure}

Figure~\ref{fig:candidates-dso-vors} presents a zoomed-in view of the same image region
for both DSO and VORS candidates selection mechanisms.
As visible on the left figure, DSO candidate points are regularly spaced,
except in homogenous areas.
VORS candidate points however tend to form contiguous lines.

\Axel{Peut-etre qu'un petit commentaire supplementaire sur ce dernier point pourrait etre
interessant ? Est-ce que former des lignes continues est une bonne propriete ?}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.49\linewidth]{assets/img/candidates-icl-dso-cropped.png}
	\hfill
	\includegraphics[width=0.49\linewidth]{assets/img/candidates-icl-vors-cropped.png}
	\caption{DSO (left) and VORS (right) candidate points (in red).}%
	\label{fig:candidates-dso-vors}
\end{figure}

\subsection{Multi-Resolution Direct Image Alignment}%
\label{sub:multi-res-direct-image-alignment}

\subsubsection{Convergence of the Optimization Problem}%
\label{ssub:convergence-optimization}

As presented in Section~\ref{sub:appearance-based},
under the photoconsistency assumption, direct image alignment consists
in finding the warp function $W$ minimizing
\[
	\sum_{\bm{x}}\|I(W(\bm{x})) - I^{*}(\bm{x})\|^2
\]
where $I^{*}, I$ are the reference and new images,
and $\bm{x}$ is the position of a pixel in the reference image.
As explained in Baker and Matthews~\cite{baker2004lucas},
this can be minimized with an iterative optimization.
If we note $\bm{\xi}$ the parameters of the warp function,
and $\delta\bm{\xi}$ the iterative increments,
the expression of the residual in an inverse compositional formulation is
\[
	I^{*}(W(\bm{x}, \delta \bm{\xi})) - I(W(\bm{x}, \bm{\xi})).
\]
In a Gauss-Newton scheme, the iterative increment $\delta\bm{\xi}$ is computed as
\[
	\delta \bm{\xi} = \inv{H} \sum_{\bm{x}} \tr{J} (I^{*}(\bm{x}) - I(W(\bm{x}, \bm{\xi})))
\]
where $J = \nabla I \cdot J_{\bm{\xi}}W$, $\nabla I$ is the image gradient
and $J_{\bm{\xi}}W$ is the Jacobian of the warp function.
The Hessian is computed as the Gauss-Newton approximation
$H = \sum_{\bm{x}}\tr{J}J$.
We parameterize $W$ in the Lie algebra of twists $\seee$.
The expression of the Jacobian of the warp function
is detailed in Appendix~\ref{sec:notation}.

In theory however, this iterative algorithm is only correct under the assumption
that the initialization is already near the solution.
Convergence to the correct minimum is thus a hard problem
and under some circumstances, the optimization may drift to another local minimum.
One way to help convergence is to use the Levenberg-Marquardt approximation
of the Hessian.
It consists in multiplying the diagonal elements of
the Gauss-Newton approximation of the Hessian by $(1 + \lambda_{lm})$.
The Levenberg-Marquardt coefficient $\lambda_{lm}$ is dynamically
updated toward 0 when converging or toward $+\infty$ when diverging.

Another method to improve convergence consists in solving the optimization
with a coarse-to-fine multi-resolution approach.
Indeed, the image gradient used for the Jacobian contains information
of larger areas of the original image when computed at lower resolutions.
As a consequence, the vicinity of the global optimum is artificially increased.
For the direct image alignment, we thus use a pyramid of images,
where each level has half the resolution of the previous one.
As explained previously the number of levels also impacts candidate points selection.
Indeed, at the lowest resolution, all pixels serve as candidates for the optimization.
The number of levels is thus chosen as a compromise between the minimum amount
of candidate points and the desired convergence rate.
Starting from 640x480 images in the ICL-NUIM and TUM RGB-D datasets,
we found that 6 levels is a good compromise.
The lowest resolution has 20x15 images, which implies a minimum of 300 candidate points.

\subsubsection{Multi-Resolution Intrinsics}%
\label{ssub:multires-intrinsics}

Since we use a coarse-to-fine optimization,
we must be able to initialize one level from the result of the previous one.
In Section~\ref{sub:intrinsic_parameters}, we presented the image formation as
the succession of two transformations, first the projection from the camera frame
to the image frame, and then the projection onto the pixels frame.
When halving the image resolution, the second transformation $K_s$ changes.
\[
	K_s = \begin{pmatrix}
		s_x & s_{\theta} & o_x \\
		0 & s_y & o_y \\
		0 & 0 & 1 \\
	\end{pmatrix}.
\]
We note $K_s'$ the new pixels projection with a resolution multiplied by $\alpha$.
In our case $\alpha$ is of the form $2^{-n}$.
The scaling parameters are all multiplied by $\alpha$ i.e.\
$s_x' = \alpha s_x$,
$s_y' = \alpha s_y$ and
$s_{\theta}' = \alpha s_{\theta}$.
The principal point parameters on the other hand depend on the coordinate system.
In most modelizations, the value of pixel $(x,y)$ is interpreted as the integration
of the light hitting the sensor on the surface located between $(x-0.5, y-0.5)$
and $(x+0.5, y+0.5)$.
The coordinates of the top left corner of the image is thus $(-0.5, -0.5)$
instead of $(0,0)$.
Figure~\ref{fig:multiscale-intrinsics} illustrates how
the principal point is transformed in such coordinate system.
As a result, the location of the principal point is updated as
$o_x' = \alpha(o_x + 0.5) - 0.5$ and
$o_y' = \alpha(o_y + 0.5) - 0.5$.
In the end, the updated intrinsic pixel transformation is
\[
	K_s' = K_{\alpha}K_s \quad \text{with} \quad
	K_{\alpha} = \begin{pmatrix}
		\alpha & 0 & 0.5(\alpha - 1) \\
		0 & \alpha & 0.5(\alpha - 1) \\
		0 & 0 & 1 \\
	\end{pmatrix}.
\]

\begin{figure}[ht]
	\centering
	\includegraphics[width=\linewidth]{assets/img/multiscale-intrinsics.pdf}
	\caption{Effect of a centered coordinate system on
	the coordinates of the principal point at multiple resolutions.}%
	\label{fig:multiscale-intrinsics}
\end{figure}

The presented approach is the one used by the TUM RGB-D coordinate system
for the principal point given in the intrinsics matrix.
It is thus also the one we use when initializing the multi-resolution intrinsic
parameters in step 2 of Figure~\ref{fig:overview-vors}.
The camera motion estimated at one resolution can thus be reused as-is
for the initialization of the next resolution.
It is the projection that changes, due to a different intrinsic matrix.

\subsubsection{Keyframe Update}%
\label{ssub:keyframe-update}

We mentioned in VORS overview that the decision to change the keyframe is done heuristically.
Similarly to ORB-SLAM~\cite{mur2015orb} and DSO~\cite{engel2017direct},
we use the optical flow of tracked points, which is
the distance in pixels between their locations in the reference and in tracked images,
to make that decision.
For convergence reasons, we established that the reference and the tracked frames
should not be too different from each other.
We therefore use a mean threshold distance of one pixel at the lowest image resolution.


\subsection{Limits of the Implementation}%
\label{sub:limits-implementation}

To our knowledge,
VORS is the first Rust-only complete direct visual odometry stack.
We thus value simplicity for this important milestone.
Incidentally, our algorithm lacks a few significant features,
left as later improvements.

One notable missing component is robustness to outliers.
It is common that the photoconsistency assumption does not hold with real-world images.
Among the many possible reasons, two of them are the presence of dynamic moving objects,
and the appearance of bright spots due to light reflections.
One possible solution is to use a robust M-estimator instead of a least square estimation.
The energy to minimize then takes the form
\[
	\sum_{\bm{x}}\rho(I(W(\bm{x})) - I^{*}(\bm{x}))
\]
where $\rho$ has a few interesting properties.
In part 2 of their series on direct image alignment~\cite{baker2003lucas},
Baker et al. explain in details how a robust M-estimator can be used
within an inverse compositional iteratively reweighted least squares (IRLS) algorithm.
Possible estimators include Huber, Geman-McClure, Tukey or even t-distribution estimators.
Klose et al.~\cite{klose2013efficient} use Huber and Tukey M-estimators,
while for example, Kerl et al~\cite{kerl2013robust}
and Gutierrez et al.~\cite{gutierrez2015inverse} prefer the t-distribution.

Another commonly appearing problem with cameras is automatic exposure variations.
With changes in lighting conditions,
exposure parameters of cameras are often automatically adjusted,
resulting in global photoconsistency errors.
Different affine exposure parameterizations have successfully been integrated
in the expression to minimize, such as in~\cite{klose2013efficient} and~\cite{engel2017direct}.
We did not add such additional parameters in our modelization
and thus expect VORS to have difficulties tracking the camera motion
in scenes with brightness changes.


\subsection{Port of VORS to WebAssembly}%
\label{sub:vors-port-wasm}

(1) how to get/capture images in a Web browser -> usage of a tar archive
(2) performances issues with realtime PNG decoding


\section{RGB-D Visual Odometry Evaluation}%
\label{sec:rgbd-vo-evaluation}

As previously explained, our implementation
belongs to the family of direct visual odometry from RGB-D images.
In this section, we will detail how it has been evaluated against
comparable algorithms by introducing the available datasets,
presenting the different evaluation metrics,
and finally lay out the testing setup we provide with the evaluation results.

\subsection{Dataset Creation / Acquisition}%
\label{sub:dataset_creation}

Comparing different algorithms is a complex task for many reasons,
one of them being the ability to run those algorithms on the same set of data.
The existence of well built reference datasets is thus a crucial point,
and understanding their characteristics is valuable to correctly compare and interpret
evaluation results.

\subsubsection{Overview of Avaliable Datasets}%
\label{ssub:datasets_overview}

We saw in Chapter~\ref{cha:the_image_annotation_problem} that the principal
difficulty for building annotation datasets is the required human annotation time.
For visual odometry, algorithms are strongly related to capture devices,
be it stereo, mono, RGB-D cameras, or cameras paired with other sensors
such as inertial measurement units (IMU), GPS or lidar.
As a consequence, different datasets focus on different acquisition systems.
For every evaluated acquisition system, there must exist another measurement system,
more precise and reliable than the one being evaluated.
The main difference is thus that
the challenge is technological for visual odometry,
while it is mostly time consumption for image annotation.

The availability of datasets for visual odometry first came from the mobile robotics community,
mostly interested in SLAM from laser sensors (lidar).
The data is thus collected from sensors attached to a mobile robot or a car.
In the New College~\cite{smith2009new} and NCLT~\cite{carlevaris2016university} datasets,
the robotic platform is based on a Segway, the KITTI dataset~\cite{geiger2013vision}
recorded data from a sensors equipped car,
while the EuRoC MAV~\cite{burri2016euroc}
dataset is based on a micro aerial vehicle (MAV).
These platforms are depicted in Figure~\ref{fig:mobile-robot-slam}.
The ground truth was recorded with three different approaches for these datasets.
Visual odometry ground truth was an after thought for the New College dataset,
only available a year later on their website and not discussed in the paper.
It appears to have been obtained from dead-reckoning, i.e.\ from wheel and IMU odometry,
provided by the Segway platform, and is thus not very reliable.
In the NCLT dataset, the mobile robot trajectory ground truth is computed from
a high precision realtime kinematic GPS (RTK GPS) and a graph SLAM based on lidar measurements.
The accuracy of the trajectory ranges from a centimeter to a decimeter approximately for a total
travelled distance of roughly 147 km.
The ground truth trajectory of the KITTI dataset was also obtained from high precision
GPS/IMU sensors and is thus also accurate at the decimeter scale.
The setup for the EuRoC MAV dataset is a bit different since the mobile vehicle
is flying in indoor environment and its trajectory is obtained from motion capture devices.
The total travelled distance is thus way shorter, less than a kilometer,
but the trajectories are accurate at approximately a millimeter.

\begin{figure}[ht]
	\centering
	\includegraphics[height=0.3\linewidth]{assets/img/new-college.png}
	\hfill
	\includegraphics[height=0.3\linewidth]{assets/img/nclt.png}
	\hfill
	\includegraphics[height=0.3\linewidth]{assets/img/euroc-mav.png}
	\caption{Mobile robots used for SLAM datasets. From left to right,
	the Segway platform used in the New College dataset~\cite{smith2009new},
	the Segway platform used in the NCLT dataset~\cite{carlevaris2016university},
	the MAV used in the EuRoC MAV dataset~\cite{burri2016euroc}.}%
	\label{fig:mobile-robot-slam}
\end{figure}

A second wave of datasets,
represented in bold in Table~\ref{tab:vo-datasets},
has especially been targetting RGB-D cameras,
becoming popular after the launch of the Microsoft Kinect.
While the TUM RGB-D~\cite{sturm2012benchmark} and Bonn RGB-D~\cite{palazzolo2019iros}
datasets are recorded with real RGB-D sensors,
the ICL-NUIM~\cite{handa2014benchmark} dataset was generated (ray-traced rendered)
from synthetic 3D models of indoor scenes.
We are going to explain in more details the specificities of the TUM RGB-D and
ICL-NUIM datasets since these are the two we used to evaluate our direct RGB-D
visual odometry algorithm.

Finally, with the popularity of inertial sensors coupled with cameras in
modern smartphones enabling new augmented reality functionalities,
a regain in interest has been visible for 6 degrees of freedom visual inertial odometry (VIO).
While the EuRoC MAV~\cite{burri2016euroc} and TUM VI~\cite{schubert2018tum} datasets
are using high quality sensors,
the ADVIO dataset~\cite{cortes2018advio} is actually using regular smartphones sensors,
showing the importance of datasets with less precise data to improve algorithms robustness.
We provide a brief summary of these datasets properties in Table~\ref{tab:vo-datasets}.
Note that this is not an exhaustive list of available datasets but an overview
of the main ones for visual odometry.

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{llll}
Dataset
	& Year
    & Available data
	& Ground truth \\
    \midrule
New College~\cite{smith2009new}
	& 2009
	& \makecell[l]{GPS, IMU, wheel odometry,\\lidar, omnidirectional, stereo}
	& Dead-reckoned \\
\textbf{TUM RGB-D~\cite{sturm2012benchmark}}
	& 2012
	& IMU, \textbf{RGB-D}
	& Motion capture \\
KITTI~\cite{geiger2013vision}
	& 2013
	& GPS, IMU, lidar, stereo
	& High precision GPS/IMU \\
\textbf{ICL-NUIM~\cite{handa2014benchmark}}
	& 2014
	& \textbf{RGB-D}, 3D surface
	& Synthetic \\
NCLT~\cite{carlevaris2016university}
	& 2016
	& \makecell[l]{GPS, IMU, wheel odometry,\\lidar, omnidirectional}
	& RTK GPS and lidar SLAM \\
EuRoC MAV~\cite{burri2016euroc}
	& 2016
	& Stereo camera
	& Motion capture \\
TUM Mono~\cite{engel2016photometrically}
	& 2016
	& Mono camera
	& Closed loop \\
TUM VI~\cite{schubert2018tum}
	& 2018
	& IMU, Mono camera
	& Motion capture and closed loop \\
ADVIO~\cite{cortes2018advio}
	& 2018
	& Smartphone IMU and video
	& IMU with position fixes \\
\textbf{Bonn RGB-D~\cite{palazzolo2019iros}}
	& 2019
	& IMU, \textbf{RGB-D}, lidar
	& Motion capture \\
\end{tabular}%
} % end of resizebox
\caption{Visual odometry datasets}%
\label{tab:vo-datasets}
\end{table}


\subsubsection{Motion Capture for RGB-D Visual Odometry}%
\label{ssub:motion_capture}

The TUM RGB-D dataset~\cite{sturm2012benchmark} was the first
complete and rigorously detailed dataset for RGB-D visual odometry.
It is composed of 80 sequences, 47 for training with ground truth,
and 33 for validation only evaluated online, without ground truth.
The training sequences are arranged in six groups,
\begin{itemize}
\setlength\itemsep{-0.5em}
	\item testing and debugging (4 sequences), simple translation or rotation movements,
	\item handheld movements (11 sequences),
	\item robot movements (4 sequences),
	\item structure and texture (8 sequences), with difficult structure or texture patterns,
	\item dynamic objects (9 sequences),
	\item and object reconstruction (11 sequences).
\end{itemize}
As we can see, the dataset provides both easy sequences and sequences with
more difficult situations such as dynamic movements or poor textures
in the field of view of the camera.
It is thus a good benchmark of the robustness of visual odometry algorithms.
These sequences have been acquired by three different Kinect devices,
named fr1 (for ``Freiburg 1''), fr2 and fr3.
All their intrinsic parameters are available in the dataset.
The required robustness of the tested algorithms is also increased by the fact that
the color camera of the Kinect sensor has a rolling shutter.
We do not expect our algorithm to perform very well under those circumstances.

The ground truth camera poses are obtained thanks to an external motion capture system
based on MotionAnalysis~\cite{MotionAnalysis} hardware and software.
This setup is composed of eight 300 Hz Raptor-E high definition cameras,
equipped with infrared lights to illuminate passive markers attached to the Kinect.
After a detailed intrinsic and extrinsic parameters calibration of the system,
the authors estimate the relative position errors to be lower than 1 mm and 0.5 degrees.

\subsubsection{Synthetic Dataset Creation}%
\label{ssub:synthetic_dataset}

Most of the available visual odometry datasets lack a dense 3D surface ground truth
to be able to evaluate both the camera trajectory and the structure reconstruction.
To this end, Handa et al.\ created the ICL-NUIM dataset~\cite{handa2014benchmark}.
Contrary to most other datasets, the video sequences provided here are completely
generated by computer graphics, using the open-source ray tracing algorithm
POV-Ray~\cite{povray}.
The dataset is split into two rooms, a living room and an office,
and two scenarios, one noiseless and one with simulated noise.
The full pipeline is also provided as open source,
if one wishes to customize parts of it.
The geometry and some renders of the living room are displayed in Figure~\ref{fig:icl-nuim}.

\begin{figure}[t]
	\centering
	\includegraphics[height=0.3\linewidth]{assets/img/icl-nuim-geometry.png}
	\hfill
	\includegraphics[height=0.3\linewidth]{assets/img/icl-nuim-povray.png}
	\caption{Geometry (left) and few rendered images (right)
	of the living room used for the ICL-NUIM dataset~\cite{handa2014benchmark}.}%
	\label{fig:icl-nuim}
\end{figure}

In order to test some visual odometry algorithms,
we are only going to use the eight noise-free sequences since realistic noisy sequences
are already evaluated with the TUM RGB-D dataset.
The trajectory ground truth generated by this ICL-NUIM dataset is thus error-free.
The color and depth images are also perfectly aligned,
timely synchronized and have no light exposure variations.
They still contain reflective surfaces and few other ligthing effects
not taken into account in our direct visual odometry modelization,
but the overall circumstances should be very favorable for our simple implementation.

\subsection{Evaluation Metrics}%
\label{sub:metrics}

There are basically two types of evaluation metrics,
those based on a ground truth, and those that are not.

\subsubsection{Metrics with Ground Truth, ATE and RPE}%
\label{ssub:metrics_gt}

The most straightforward method to evaluate a visual odometry trajectory
is to compare it with a reference trajectory, called ground truth.
This reference trajectory needs to be acquired with more precision
than the one being evaluated for the measure to make sense.
In our case, the TUM RGB-D dataset uses a motion capture system with sub-millimeter accuracy,
and the ICL-NUIM provides exact trajectories since it is a synthetic dataset.

The first evaluation metric commonly used is the absolute trajectory error (ATE).
We can consider the trajectory as a discrete set of camera poses
\[
	G_{\Omega} = \Set{g_{\tau}}{\tau \in \Omega,\ g_{\tau} \in SE(3)}
\]
where $\Omega$ is the set of discrete time events when camera poses are available.
In theory, the ATE can be computed as
\[
	\text{ATE} = \sum_{\Omega} d(g_{\tau}, \widehat{g}_{\tau})
\]
where $d(g_{\tau}, \widehat{g}_{\tau})$ can be thought of
as a distance between the estimated transformation $g_{\tau}$
and the ground truth transformation $\widehat{g}_{\tau}$.
Usually, we can consider two types of errors, the translation error
\[
	\| \text{trans}(\inv{\widehat{g}_{\tau}} \cdot g_{\tau}) \|
\]
and the rotation error
\[
	\measuredangle \, (\inv{\widehat{g}_{\tau}} \cdot g_{\tau})
\]
where $\measuredangle$ is the amplitude ($\geq 0$) of the rotation angle.
Since the rotation errors will also impact translation errors later,
it is common to only use the translation error when computing the ATE.
The two most widely used ATE metrics are the RMSE and the median scores
\[
	\text{ATE}_{\text{rmse}} =
		\left(
			% \frac{1}{\text{Card}(\Omega)} \sum_{\Omega}
			\mean
			{\| \text{trans}(\inv{\widehat{g}_{\tau}} \cdot g_{\tau}) \|^2}
		\right)^{1/2}
	\quad \text{and} \quad
	\text{ATE}_{\text{median}} =
		\text{median} \left( \| \text{trans}(\inv{\widehat{g}_{\tau}} \cdot g_{\tau}) \| \right).
\]
The median is a better indicator of the algorithm precision,
while the RMSE better reflects the presence (or absence) of outliers,
i.e.\ the global robustness of the algorithm.

In practice, we should note that the ground truth and estimated trajectory
are not expressed in the same reference frame.
It is thus necessary to first align the two trajectories.
This is usally done with a principal component analysis (PCA)
of the trajectories.
It is also important to note that the ground truth and estimated trajectories
are not sampled at the same timings and frequency.
Therefore, it is also necessary to correctly associate poses of each trajectory,
which is reasonably easy when timestamps have been synchronized in the dataset.
One of the main issues of the ATE is that drifts have higher impact at the beginning
of the sequence than at the end.
To better evaluate long sequences we thus use another metric,
the relative pose error (RPE).

Contrary to the ATE, which evaluates absolute errors on associated pairs of poses,
the RPE compares the relative motion in a sliding window along the sequence.
The size of the window is usually a fixed travelled distance or time interval.
We will detail for example the computation of the RPE at 1 second.
For each associated pair of estimated and ground truth poses $(g_{\tau}, \widehat{g}_{\tau})$,
we consider another pair $(g_{\tau'}, \widehat{g}_{\tau'})$
delayed by 1 second ($\tau' \approx \tau + 1s$).
The camera motion between $\tau$ and $\tau'$ in the estimated trajectory is
$\Delta_{\tau \tau'} g = \inv{g_{\tau}} \cdot g_{\tau'}$
and the camera motion in the ground truth trajectory is
$\Delta_{\tau \tau'} \widehat{g} = \inv{\widehat{g}_{\tau}} \cdot \widehat{g}_{\tau'}$.
The relative pose error is thus computed as
\[
	\text{RPE} = \sum_{\Omega'} d(\Delta_{\tau \tau'} g, \Delta_{\tau \tau'} \widehat{g})
\]
where $d(\cdot, \cdot)$ is similar than for the ATE,
and $\Omega'$ is the set of regularly spaced pairs,
whether it is a duration, like 1 second, or a distance like 1 meter.

\alert{Ajouter un schéma RPE avec les notations précédentes.}


\subsubsection{Metrics without Ground Truth}%
\label{ssub:metrics_no_gt}

In some conditions, such as long handheld outdoor trajectories,
retrieving a ground truth might be problematic.
Yet it is still possible to partially evaluate the precision of an algorithm,
or rather its robustness to drifts and losses.
Indeed, in absence of a full sequence ground truth,
it is still possible to evaluate the tracking of loops.
It is important to note that global loop closure detections must
be deactivated in the algorithms for these measures to make sense.

One method consists in computing loop closure and pose graph optimization
and then compare the trajectory before and after.
If the drifts of the tracking are not too extreme and do not prevent the pose graph optimization,
this error can be representative of the visual odometry performance.

Another approach consists in specifically designing the dataset sequences
to start and end at the same location, such that both ends of the trajectory
can be precisely aligned.
This is the approach taken in~\cite{engel2016photometrically}.
The error computed is then the accumulated drift between the sequence when aligned
to the start and when aligned to the end of the partial ground truth trajectory.


\subsection{Setup and Algorithms Evaluation}%
\label{sub:algorithms_eval}

In this section, we describe how we evaluated our VORS implementation
and compared it to other open source C++ algorithms.
We focused on RGB-D visual odometry, and therefore used
both the TUM RGB-D~\cite{sturm2012benchmark} and ICL-NUIM~\cite{handa2014benchmark} datasets.

\subsubsection{The TUM RGB-D Dataset Format}%
\label{ssub:dataset-format}

The TUM RGB-D dataset is well specified and as a result,
others including ICL-NUIM follow the same format.
We therefore also base our evaluation setup on the TUM RGB-D format.
The general structure of such dataset is as follows.
\lstinputlisting[%
  language=bash,
  caption={General structure of the TUM RGB-D dataset format.},
  label={lst:dataset}
]{assets/code/dataset.txt}
The files \verb|rgb.txt| and \verb|depth.txt| list all images with their associated timestamps.
The RGB-D visual odometry algorithms also need the correspondences between color
and depth images, so if not present, the first step is usually to run
a provided \verb|associate.py| script to generate an \verb|associations.txt| file
from the \verb|rgb.txt| and \verb|depth.txt| files.
Each line of that file contains a pair of RGB and depth images with their respective timestamps.
Color images are stored as 640x480 8-bit RGB images in the PNG format.
Depth images are stored as 640x480 16-bit monochrome images in the PNG format.
Depth images are scaled by a factor of 5000, i.e.\ a pixel value of 5000 corresponds
to a distance of 1 meter.
Theoretically depth images thus have a precision of 0.2 mm for a range of
0 to roughly 13~meters.
The ground truth trajectory is provided in the \verb|groundtruth.txt| file as follows.
\lstinputlisting[%
  language=bash,
  caption={Ground truth trajectory format.},
  label={lst:groundtruth}
]{assets/code/groundtruth.txt}
The timestamp is the POSIX time of the given pose,
i.e.\ the number of seconds elapsed since 1970, January 1st.
The coefficients \verb|tx|, \verb|ty|, \verb|tz|, are the coordinates
of the optical center of the color camera with respect to a given world frame.
The coefficients \verb|qx|, \verb|qy|, \verb|qz|, \verb|qw| are the parameters of the quaternion
describing the orientation of the color camera.
The last coefficient \verb|qw| is the real part of the quaternion.

\subsubsection{Open Source Algorithms and Provided Setup}%
\label{ssub:algorithms_and_setup}

In addition to VORS, we evaluated five other open source algorithms for RGB-D visual odometry,
namely DVO~\cite{kerl2013robust}, Fovis~\cite{huang2017visual},
and three variants of the OpenCV RGB-D odometry module
based on~\cite{newcombe2011kinectfusion, steinbrucker2011real}.
For each one of those six algorithms, we implemented a small program
performing the following operations (pseudo code).
\lstinputlisting[%
  language=bash,
  caption={Outline of the common RGB-D odometry program implemented for every tested algorithm.},
  label={lst:track}
]{assets/code/track.txt}
All those algorithms except VORS are C++ programs with different complexity of build,
due to dependency issues.
DVO for example would not compile anymore with recent versions of
the Sophus~\cite{sophus} library for Lie algebra
due to missing re-orthogonalization of the rotation matrix after optimization increments.
Therefore, we are providing clear installation instructions as well as
a Docker container ready for compilation of all 6 programs.
This setup is open sourced under GPL license
at \url{https://github.com/mpizenberg/rgbd-tracking-evaluation}.

\subsubsection{Evaluation Results}%
\label{ssub:eval_results}

We evaluated all algorithms both on the ICL-NUIM dataset and on the TUM RGB-D dataset,
constituting a total of 55 sequences.
As visible in Figure~\ref{fig:rpe_median_all},
our implementation (VORS) performs similarly than DVO, Fovis and OCV-RGBD.
It is coherent since these four algorithms use both the visual information of the color image
and the depth map to estimate the camera motion.
OCV-ICP however, which only uses the depth map,
is unable to track the camera movements in the majority of sequences.
The OCV-RGBD-ICP variant, which is a mixed approach minimizing both energies
of OCV-RGBD and OCV-ICP, inherits from the same tracking difficulties as OCV-ICP.\@

\begin{figure}[ht]
	\centering
	\input{assets/csv/rpe_median_all.tex}
	\caption{Distribution of the median relative pose error (RPE) at 1 second on all 55 sequences,
	composed of 47 TUM sequences and 8 ICL-NUIM sequences.}%
	\label{fig:rpe_median_all}
\end{figure}

Out of 55 sequences taken into account in the Figure~\ref{fig:rpe_median_all},
the wide majority (47) comes from the TUM RGB-D dataset.
VORS, which does not implement robust approaches,
should perform better for the synthetic sequences of the ICL-NUIM dataset than
for the real sequences of the TUM dataset.
Figure~\ref{fig:rpe_median_icl} thus presents the same plot than
Figure~\ref{fig:rpe_median_all} but restricted to the 8 ICL-NUIM sequences.
As we can see, VORS is performing remarkably in these conditions.
Other noteworthy details are exacerbated by this plot.
Both geometric odometries (OCV-ICP and OCV-RGBD-ICP) perform extremely well
thanks to the high precision dense depth maps provided by the dataset.
One should note that this precision comes at the price of time.
The first ICL sequence for example takes three times longer
with OCV-ICP than with Fovis, VORS and DVO, all comparable in speed.
It is also notable that Fovis is having more issues tracking those sequences.
It can be explained by the nature of this algorithm, which is indirect,
based on FAST features, contrary to VORS, DVO and OCV-RGBD which are
direct visual odometry algorithms.
The synthetic nature of the images, and lack of unique textures compared
to real images is degrading the detection rate of Fovis FAST descriptors.

\begin{figure}[ht]
	\centering
	\input{assets/csv/rpe_median_icl.tex}
	\caption{Distribution of the median relative pose error (RPE) at 1 second
	on the 8 ICL-NUIM sequences.}%
	\label{fig:rpe_median_icl}
\end{figure}

Instead of using the median RPE,
which pictures the overall precision of the algorithms,
Figure~\ref{fig:rpe_rmse_icl} represents the RMSE RPE,
which better reflects the robustness to outliers.
As visible in that figure, VORS has a long tail of highly
erroneous motion tracking.
High RMSE errors are better understood when looking at the
detailed RPE of one sequence, as shown in Figure~\ref{fig:rpe_icl3}.
In the end, even though our visual odometry implementation in Rust (VORS)
has robustness issues due to previously discussed challenges,
we provide a precise, state of the art RGB-D visual odometry,
easy to compile, to use and as we will develop next,
easy to port to WebAssembly.
VORS thus enables the exploration of efficient interactive
visual odometry on the Web, which we finally present next.

\alert{Parler plus de la dernière figure.
Ajouter une discussion un peu plus fournie
sur les perfs de VORS en bilan.
En transition mentionner que on va pouvour corriger certains
défauts de manière interactive.}

\begin{figure}[ht]
	\centering
	\input{assets/csv/rpe_rmse_icl.tex}
	\caption{Distribution of the RMSE relative pose error (RPE) at 1 second
	on the 8 ICL-NUIM sequences.}%
	\label{fig:rpe_rmse_icl}
\end{figure}

\begin{figure}[ht]
	\centering
	\input{assets/csv/rpe_icl3.tex}
	\caption{Relative pose error (RPE) at 1 second for the third
	living room sequence of the ICL-NUIM dataset.}%
	\label{fig:rpe_icl3}
\end{figure}

\clearpage
\section{Interactive VORS on the Web}%
\label{sec:interactive-vors}


\subsection{Interactions}%
\label{sub:interactions}

We have been experimenting with different interactive modalities.
Actually, we would have liked to experiment with multiple modalities.
Here is a screenshot of the Web application Interactive vors.

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{assets/img/todo.png}
	\caption{Interactive visual odometry on the Web}%
	\label{fig:interactive_vors}
\end{figure}

As you can see, the interface is split in three parts

\begin{itemize}
	\item The timeline
	\item The 3D visualization
	\item The image thumbnail of the current keyframe
\end{itemize}

The timeline enables movement along the temporal axis.
It is a one-dimentional control that is well known thanks to its pervasive use in video playing.
Visual Odometry has the advantage of dealing with video data,
contrary to structure from motion, where images are not guaranted to be in any specific order.
The timeline is thus very adapted for temporal navigation in the video.
The frames accessible from the timeline are only the keyframes of the video tracking.
Those are the frames that are used to track the following frames of the sequence.
The decision to consider a new frame as a keyframe follows a heuristic based on
the amplitude of the movement of the optical flow.
The objective is to optimize the number of keyframes in the video.
If the keyframe and the new frame are close enough,
the motion of the keyframe can be precisely estimated related to the keyframe.
So even small drifts happen during tracking, it is not additive until we change of keyframe.
Once a keyframe changes, all successive frames that will be tracked based on this keyframe
will consequently have an absolute position drift dependent of the drift of the previous keyframe.
So increasing the number of keyframes should reduce the risk of loosing completely the tracking,
but also increases the additive drift accumulated for each keyframe.

For every keyframe, we use the depth image and choose a limited amount of points
for the tracking of the following frames.
This is explained by the ``coarse to fine candidates points selection'' section.
The number of points used in each keyframe is variable but in the order of a thousand
to ten thousands points.
Those 3D points are stored in an array,
and used for a point cloud visualization with a library called ThreeJS.

The Rust ecosystem is still lacking in the domain of graphical user interfaces (GUI).
As of 2019, only a handful of libraries enable 3D graphics,
often as bindings to other C++ libraries.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\alert{The following is an extract of the ``comptes rendus'' google doc}

\textbf{Work on 3D visualizations}
Pour une application interactive d’odométrie visuelle, il est primordial d’avoir une visualisation du nuage de points généré par les keyframes, et utilisé pour le tracking. Je me suis donc intéressé aux différentes approches de rendu avec Rust. Je conseille de lire cet article de Simon Heath (@icefoxen) intitulé “A Guide To Rust Graphics Libraries 2019”.

\textbf{Petit tour rapide des APIs de rendu.}

En dehors des programmeurs de pilotes de cartes graphiques, personne n'écrit directement du code pour GPU. À la place, on utilise des bibliothèques graphiques telles que OpenGL. Aujourd'hui, les 4 principales APIs graphiques utilisables sont OpenGL, DirectX, Vulkan et Metal.

OpenGL est un standard ouvert, développé par le groupe Khronos, qui a vu le jour en 1992. Des pilotes existent pour presque tous les systèmes, linux, windows, osx, etc. Son principal problème est son âge. C'est une API assez haut niveau qui n'a pas évolué favorablement en comparaison des architectures des GPUs.

DirectX (DX) est un peu l'équivalent d'OpenGL à la sauce Microsoft, disponible uniquement pour Windows et XBox. Jusque DirectX 11, les API sont très haut niveau, comme OpenGL. DirectX 12 la dernière version fait parti d'un renouveau de pilotes plus bas niveaux, tout comme Vulkan et Metal, qui tirent beaucoup mieux parti des architectures actuelles des cartes graphiques.

Vulkan est à OpenGL ce que DX12 est à DX11, une API bien plus bas niveau mais offrant des bien meilleures performances. J'ai le sentiment que sur des appareils de faibles puissance, tels que des smartphones, la différence se fera bien ressentir et il sera courant d'avoir des perf x2, x3 ou plus suivant les cas.

Metal c'est le DX12 d'Apple. Une API bas niveau uniquement accessible sur des machines Apple.

\textbf{Et Rust dans tout ça ?}

En Rust, il y a des wrappers pour chacune des APIs graphiques, souvent le nom de l'API suffixé "-rs" (pour rust). Ces bibliothèques ont souvent une API "unsafe", ce qui signifie en Rust qu'elles n'ont pas toutes les garanties que peut apporter le langage en terme sûreté mémoire. La bibliothèque probablement la plus important de l'écosystème graphique de Rust s'appelle "gfx", aujourd'hui renommée "gfx-hal" (pour hardware abstraction layer). C'est une API bas niveau, à 97\% similaire à Vulkan mais qui est multi-plateforme ! Elle peut compiler vers Vulkan, DX12, ou Metal suivant l'environnement (et un support réduit d'OpenGL). Il existe aussi une bibliothèque très active du nom de "rendy" qui se veut être un toolkit pour gfx-hal et réduit la quantité de boilerplate nécessaire pour des APIs autant bas niveau.

\textbf{Et le Web dans tout ça ?}

Et bien justement, on est là aussi au bord d’un renouveau complet. Tout comme WebGL qui a apporté une version simplifié et sécurisée d'OpenGL sur le Web en 2011, le W3C, en collaboration avec Apple, Mozilla, Microsoft et Google, est en train de standardiser une API dédiée au Web, du nom de WebGPU. WebGPU devrait exposer pour JavaScript et WebAssembly une API graphique et de calcul (comme Cuda / OpenCL) efficace pour tout appareil ayant des pilotes natifs pour Vulkan / DX12 / Metal.

L'implémentation de WebGPU en Rust, qui s'appelle "wgpu", a démarré vers Septembre 2018. La où ca devient encore plus intéressant, c'est qu'elle est basée sur gfx-hal pour le rendu. Utiliser wgpu permet donc en théorie d'avoir un code efficace, un peu plus haut niveau que Vulkan, compilable vers toutes les plateformes natives et le Web ! En pratique malheureusement, au jour d'aujourd'hui (aout 2019), seul Chrome canary sur OSX supporte WebGPU. On ne peut donc pas (encore) s'en servir pour des applis Web. Mais vu la motivation des différents partis, ça devrait arriver vite !

\textbf{C’est beau mais en pratique on fait quoi nous ?}

De l'OpenGL / WebGL mon capitaine ! J'ai donc cherché dans l'écosystème Rust, et je suis tombé sur Kiss3D, par le même créateur que la bibliothèque d'algèbre linéaire que j'utilise. Kiss3D promet une API simple et compatible avec WebAssembly. Après quelques essais ratés, les premières visualisations correctes sont encourageantes ! Je raconte mes pérégrinations dans ce post du forum rust pour de plus amples détails.


https://youtu.be/hCzT47eeges


Mais (comme toujours il y a un mais ...) malheureusement la compilation vers wasm fait drastiquement chuter les performances sur mon ordi portable. On passe d'environ 60 fps en natif à environ 10 fps en web, ce qui n'est clairement pas utilisable. Il y a également une deuxième chose qui m'a chagriné. Kiss3D est basé sur "stdweb" pour sa compilation vers wasm. Or "stdweb" est une approche alternative à l'approche standardisée pour wasm en Rust, qui s'appelle wasm-bindgen, et les deux ne sont pas facilement compatibles. Il est donc difficile de faire cohabiter le code de tracking, utilisant wasm-bingen, et le code de visualisation, basé sur stdweb.

\textbf{Finalement on fait quoi du coup ?}

L'approche qui me donne le meilleur compromis de performances et de flexibilité, c'est de mixer les calculs faits en Rust + wasm (en utilisant wasm-bindgen), avec le rendu fait en JavaScript avec Three. Three c'est probablement la bibliothèque de 3D la plus mature pour le Web, avec une API simple en JavaScript. C’est celle que Thomas utilise pour ses rendus. En faisant tous mes calculs avec Rust, je peux partager avec Three un pointeur vers la position dans la mémoire wasm du résultat des calculs. Le seul léger inconvénient, c'est que toute donnée nécessaire pour le rendu Three, doit avoir un attribut permanent associé dans le code Rust. C'est un moindre mal, et en utilisant correctement l'API Three, j'obtiens 25 à 30 fps pour un million de points avec mon ordi portable. Axel a un solide 60 fps sur son ordi.

Voici la démo, et le code source pour un simple exemple générant un cube rempli d'un million de points aléatoires. Je suis curieux de savoir à quel framerate cet exemple tourne dans différents types d'appareils mobiles. Avec mon android bas de gamme, j'obtiens 4 fps sous chrome, et 3 fps sous firefox.

La démo la plus intéressante, c'est celle de l'appli de tracking vors ! Elle est disponible ici, code source par là. ATTENTION, pour la faire tourner, il faut 1.6 Go de ram disponible avec le dataset ICL NUIM. Pour démarrer la démo, il faut lui donner une archive tar contenant tout le dataset à tester. Il y a une archive (700 Mo) toute prête, téléchargeable jusqu'au 29 aout par ici. Ci-dessous une capture d’écran de l’appli qui tourne sur mon ordi portable.

\alert{End of the google doc extract.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As we explain in the extract above,
we ended using ThreeJS for the 3D visualization.
Upon navigation in the timeline, there are two other different modifications of the visualization.
First the 3D points corresponding to the current keyframe are displayed in another color (red),
and the camera point along the camera trajectory is updated.
This makes following the camera trajectory and identifying the associated geometry trivial.
And second, the thumbnail of the current keyframe is updated.
As a consequence, looking at the thumbnail area while moving along the timeline
reminds temporal navigation in traditional 2D videos.
It also facilitates the association of the 3D geometry with the actual 2D frame
from the original video sequence.
In sum the combination of a keyframe timeline with associated thumbnail
and 3D geometry bridges the traditionally difficult gap between 2D and 3D user interfaces.

We just have introduced the first interactive modality, the keyframe timeline navigation.
In order to introduce the other two, we need to describe first
a problem we are facing with our visual odometry algorithm.
The video sequences used for the benchmark of the different visual odometry algorithms
have different difficulty levels of tracking.
The synthetic sequence from the ICL NUIM dataset
have the particularity of providing low textured but crisp images with perfect alignment
between the color images and depth map.
Those conditions enable high precision tracking, except for some short sections
almost not textured at all, where tracking is lost and drifts.
When running interactive vors on this dataset,
we observe two of those distinctive drifts, resulting in multiple
disjoint and unaligned pieces of the global 3D model of the associated room.

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{assets/img/todo.png}
	\caption{Misaligned 3D model due to drifts in the tracking.}%
	\label{fig:drift_icl_nuim}
\end{figure}

If we set for example the objective to obtain a globally coherent 3D model of the room,
there are plenty of approaches heading in that direction.
Here are two of those.

\begin{itemize}
	\item Identify in the timeline each section with a coherent 3D model associated.
		Export a model of each temporally coherent section.
		Join all parts of the model with an approch such as ICP (Iterative Closest Point).
	\item Identify two keyframes in the timeline with a visual overlapping
		but no 3D geometry overlapping due to a drift between those images.
		Re-align those two images.
		Adapt the tracking of the rest of the sequence to take into account this adjustment.
\end{itemize}

We decided to take the second approach for this experiment.
The second option here is very similar to what we usually call SLAM.
SLAM consists in adding loop closure and pose graph optimization techniques
to visual odometry to bring global map consistency.
In traditional SLAM algorithms, points of interests are retrieved for all keyframes.
Their descriptors are classified with techniques like ``bag of words''.
Then typical indexing and search techniques are applied to identify similar keyframes.
In our case, the identification is performed by a human interaction.
Once two keyframes are matched, the next step consists in computing
the camera motion between those.
Usually, one would match all keypoints in the pair of images
and use a robust version of the 8-point algorithm if there is no depth info,
or a robust PNP algorithm if depth info is know.
Logically, we thus tried to perform keypoints detection and matching for the pair of keyframes.
There exist many keypoint descriptors for this task.
Some well known are ORB, SIFT, FAST, A-KAZE.
The A-KAZE features were already implemented in Rust (https://github.com/indianajohn/akaze-rust)
so we tried to use them for the matching.
Unfortunately, due to the low textured images of the synthetic dataset,
the number of matches for selected pairs of keyframes were in the order of 20,
not enough to compute reliable camera motion.
This is the origin of the next two interactive modalities explored.

Once a pair of keyframes is identified by a user,
thumbnails of those two frames are displayed next to each other.
The reference keyframe thumbnail also displays in red the points used for the tracking,
i.e. points which also provide depth information.
We consciously restrict the number of points with depth information to candidates
points used in the tracking, instead of all points with depth information from the sensor,
for two reasons.
(1) Those points are the only points supposed to be known if we extend our work to RGB visual odometry,
(2) less red points means less visual clutter for the user.
In those two thumbnails, the user is asked to identify three pairs of points
by clicking inside the two images.
In the reference image, the points chosen should be restricted to those in red.
From the three pairs of points, we can compute 0 to 4 potential camera positions
based on the P3P algorithm.
Since there was no currently available Rust P3P implementation,
and it was not too complicated to implement, we ported M. Persson ``Lambda Twist''
P3P algorithm. It is available at https://github.com/rust-photogrammetry/p3p.
Those potential camera positions are used to backproject the points with known depth
(red points in the thumbnail) into the 3D environment.
Every potential camera pose is associated with 3D points of a unique color
as depicted in Figure~\ref{fig:p3p}.

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{assets/img/todo.png}
	\caption{P3P camera pose candidates.}%
	\label{fig:p3p}
\end{figure}

As also visible in Figure~\ref{fig:p3p}, a probability is associated with every potential pose.
Those probabilities are estimated by a measure of photometric reprojection error.
Indeed, for each potential camera pose, it is possible to project the 3D points
of the reference keyframe into the other frame.
For this estimatation we compute the reprojection error at low resolution images
which are less sensible to small errors in the reprojection.
In the end, the probability scores are simply set proportional to the inverse of the reprojection errors.
Such scores are intended to be a hint for the user,
helping them to select the correct pose initialization in the interface.
With this combination of colored 3D visualization and selectable options,
we avoid the complex task of directly interacting in the 3D world.
In the end, the chosen pose is set as initialization to the tracking algorithm.
The tracking is then restarted from this second keyframe.
It should provide a globally consistent 3D model until the next drift in the sequence.

\subsection{Drawbacks of the approach}%
\label{sub:drawbacks}

In traditional SLAM, the loop closure and pose estimation would add
an edge in the global pose graph of the sequence.
Then a pose graph optimization algorithm would be run to improve the global
consistency of the pose graph.
In our simplified case however, we do not build this pose graph,
but simply ensure the coherence of the sequence after the second keyframe
with regard to the reference keyframe.
It means that all frames in between the pair of keyframes is not adjusted.
In particular, there will still be drifted frames in this section of the sequence.

\subsection{Code architecture of interactive VORS}%
\label{sub:code-interactive-vors}

The code of this project is available at https://github.com/mpizenberg/interactive-vors.
As of version 1.0.0, it is composed of three main parts.
Those are clearly reflected by the language percentage usage in GitHub Web page interface,
38\% JavaScript, 35\% Elm and 26\% Rust.

The Elm application manages the Web user interface and communicates
with the JavaScript part to handle the 3D drawing and visual odometry.

The 3D rendering is handled by a JavaScript library called ThreeJS, based on WebGL.
A keyframe may contain 1 to 10 thousand points with depth information.
In a typical tracking situation, there might be one keyframe for rougly 5 to 20 frames.
The ICL-NUIM sequence for example, is composed of 1508 frames,
and results in 225 keyframes and 550000 points.
Being able to display point clouds with a number of points in the order of the million
with WebGL is still a non trivial task for today mid-end hardware.
In order to mitigate 3D drawing performance bottlenecks,
all the 3D visualizations in the application are based on a unique preallocated
geometry buffer sized for a million point.
While it is a reasonable solution for our proof of concept use case,
this is an important detail if we are to consider more memory efficient solutions.
We will also note, that with the advent of the new standard WebGPU,
better performances and compatibility with WebAssembly are to expect in the near future.

The visual odometry algorithm is of course a WebAssembly module,
compiled from a small Rust interface module,
reusing our Rust visual odometry algorithm (vors).
Our JavaScript code base is communicating with this Wasm module
thanks to the intermediate JavaScript binding code generated by wasm-bindgen and wasm-pack.
Before diving into the details of porting vors to WebAssembly,
we will provide few quantitative measures of the improvements that our interactive
approach enabled.

\subsection{Quantitative improvements of the 3D model and trajectory}%
\label{sub:quantitative-improvements}

(1) absolute trajectory error
(2) absolute point cloud error
