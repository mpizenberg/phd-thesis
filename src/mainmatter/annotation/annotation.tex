\chapter{The Image Annotation Problem}%
\label{cha:the_image_annotation_problem}

\adjustmtc
\minitoc%

In this chapter, we introduce the concept of image annotation, and review the body of work that have been researched in this domain. 

But first, what is image annotation? The most simple way to put it would be to consider it as the process of augmenting an image with information. This information can be of various nature (we review the different types annotation in section ???), and is typically provided by a human operator, also called annotator. 

We could consider photogrammetry as the first historical example of image annotation, long before digital images even existed. The process of measuring distances and lengths of the real world from 2D images requires annotating these distances and lengths in the image space, before inferring the values in 3D. Similarly, early techniques in the old cinema involved manually editing the filmstrip to create special effects, which is a form of annotation.

Digital imaging has progressively brought new needs for image annotation. The first digital image was scanned from a photograph in 1957 by Russell Kirsch, and the first digital camera was built in 1975 by the Kodak engineer Steve Sasson. Commercial models of digital cameras became really available in the 1990s, and from then the volume of digital images produced grew exponentially every year.

In the meantime the computer vision community, which originated from the artificial intelligence one, became more and more interested in machine learning. Machine learning designates a class of algorithms in which a model learns from experience, materialized by data samples. One sub-domain of machine learning, called \textbf{supervised} machine learning, requires in particular annotated samples, meaning that a label should be assigned to each piece of data before an algorithm can be trained to predict these labels. Supervised machine learning gained traction in the 1990s during which some applications reached high enough maturity to be exploited commercially. A famous example of this is the digit recognition algorithm from Lecun et al. \cite{blabla} which was used by AT\&T to automatically process cheques in ATMs. A nowadays popular dataset, called MNIST (Mixed National Institute of Standards and Technology) was created for this work; this dataset associates labels (digits, from 0 to 9) to $28\times 28$ pixel images of handwritten figures. 

This dataset illustrates how image annotation could be used to produce desirable applications, and is only a small example of what has now become a classic pipeline to solve problems in the computer vision community. Note that while we focus this chapter on computer vision (because image annotation has become key in this community), the machine learning pipeline we mention is also used in many other problems such as audio processing or natural language processing. 

Theoretical results in machine learning postulate that problems of great complexity could be adressed with this technique, provided that (i) there exists a model of sufficient capacity to cope with the problem complexity, and (ii) a sufficiently large sample of annotated data is available. Some thresholds have been established by the community to estimate what "sufficiently large" means \cite{blabla}, but computer vision problems typically requires millions of (annotated) images to be solved with an acceptable performance. Models relying on deep neural networks are nowadays the most popular techniques in machine learning, but other models (such as deep random forests for example, used in the human body pose estimation embedded in the Kinect \cite{blabla}) may still be considered depending on the application. Note that while gathering more and more data is the current trend in computer vision, an important field of research conversely focuses on learning on few samples; this field regroups the notions of semi-supervised learning, weakly-supervised learning, one-shot and few-shots learning, etc. 

In what follows, we will review the computer vision problems for which large datasets, often combined with (deep) machine learning techniques, have recently significantly improved the state-of-the-art. We will then present a list of possible annotations that can be added to an image. 

\subsection*{Computer vision problems that require annotation}
  
Among all the computer vision problems that people have tried to solve over the years, what could be considered as the simplest one is \textbf{image classification}. Classifying an image consists in assigning it one, or several, label.s. that describe either the type of scene it depicts (interior or exterior ; beach, mountain, forest, city etc.), or the objects that are displayed, in a certain order of importance. This task requires an advances understanding of the images, and has been a well-studied topic that could now be considered solved. 

In fact, two of the most important challenges in computer vision have highlighted this task as one of the main problem to be solved. The first one, Pattern Analysis, Statistical Modelling and Computational Learning Visual Objects Challenge, often called PASCAL VOC, has run from 2005 to 2012 and figured at its peak 11,530 images depicting 20 classes. It is interesting to note that this challenge coincides in time with the rise of machine learning popularity in the computer vision community. In a way, PASCAL VOC has been both a marker and a catalyzer of the importance of machine learning in image processing problems. PASCAL VOC stopped in 2012 sadly due to the passing of one of its most invested organizers, Mark Everingham, as well as due to the growing importance of a much larger challenge: the ImageNet Large Scale Visual Recognition Challenge (ILSVRC).

ILSVRC started in 2010, motivated by the goal to solve larger scale problems. ILSVRC relies on a gigantic dataset called ImageNet, that originally intended to match a natural language dataset called WordNet. WordNet is a database of english words, grouped into sets of synonyms called synsets. The goal of ImageNet is to provide a set of images to describe each of these synsets. As of December 4th 2019, ImageNet displays 14,197,122 images that depict 21,841 different synsets.

We should also mention a parallel effort funded by the Canadian Institute for Advanced Research, which led to the creation of CIFAR-10 and CIFAR-100. Those datasets contain images of size $32 \times 32$ collected over various Web images searching tools, and classified under 10 and 100 classes respectively. ImageNet and CIFAR-100 are often both used to assess the performance of the classifiers.

While many subsequent work have further improved state-of-the-art classification results, we could synthesize the progress in classification by citing two papers. The first one from Krizhevsky et al. in 2012, nicknamed AlexNet, was probably key in the rise of Deep Learning that followed. AlexNet won the ILSVRC 2012 challenge by a large margin, starting the trend of using deep neural networks to solve computer vision problems. The second paper from He et al in 2015 and often called ResNet, introduced residual blocks (through skip connections) to ease the training of very deep neural networks (up to 1,000 layers!). The 152 layers version of ResNet won the ILSVRC 2015 challenge by reaching an error rate so low that it could be considered below the average human performance. The general trend in subsequent work has been to reach comparable (and even higher) performance than ResNet while reducing the number of parameters and operations to a minimum.  

\vspace{0.5cm}

\begin{center}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Dataset & \# classes & \# images & image size & annotation process \\
		\hline
		PASCAL VOC & 20 & 11k & any & Mechanical Turk \\
		ImageNet & 21k & 14M & any & Mechanical Turk \\
		CIFAR-10 & 10 & 60k & $32 \times 32$ & recruited students \\
		CIFAR-100 & 100 & 60k & $32 \times 32$ & recruited students \\
		SUN397 & 397 & 130k & any & LabelMe \\
		ESP Game & any & 100k & any & the ESP Game \\
		\hline
	\end{tabular}
\end{center}
\vspace{0.5cm}

Another important topic, that extends in a sense the image classification problem, is the one of \textbf{image captioning}. It consists in describing an image with a set of sentences. Image captioning is a much harder problem than classification, because captions require a higher level of image understanding as well as natural language capabilities to generate valid sentences. In terms of annotations, it is much longer than just assigning a class to an image as well. Another difficulty for data gathering is the quality check of the annotations, since two sentences from two different users may be completely different but still convey the same semantic meaning. 

\vspace{0.5cm}

\begin{center}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Dataset & \# captions & \# images & annotation process \\
		\hline
		MS COCO \cite{chen2015microsoft} & 1.5M & 330k & Mechanical Turk \\
		\texttt{nocaps} \cite{agrawal2019nocaps} & 166k & 15k & Mechanical Turk \\
		Conceptual Captions \cite{sharma-etal-2018-conceptual} & 3.3M & 3.3M & Web crawling \\
		Flickr30k \cite{flickr30k} & 150k & 30k & Mechanical Turk \\
		\hline
	\end{tabular}
\end{center}
\vspace{0.5cm}

Different types of problem that require annotation:
\begin{itemize}
	\item Image captioning: describe an image with a set of sentences
	\item Object detection and localization : object class + bounding box
	\item Object segmentation : object pixel mask
	\item Image Segmentation. Semantic Segmentation / Image Parsing : classify all pixels in an image
	\item Human Pose annotation, shapes segmentation and similarity, etc.
\end{itemize}
All these annotations have been extended to video \\ \\

% What applications to these problems? \\ \\
%
% Different types of interactions for these annotations:
%
% \begin{itemize}
% 	\item Free text typing
% 	\item Icon clicking
% 	\item points, lines
% 	\item bounding box
% 	\item polygon
% 	\item crcles, ellipses ??
% \end{itemize}
%
% Other important points:
%
% \begin{itemize}
% 	\item implicit/explicit annotations
% 	\item Quality check of the annotations? Self-correcion system? Detection of bad annotations?
% 	\item expert or non-expert annotation ? Crowdsourcing ?
% 	\item QoE of the interaction staken into account?
% \end{itemize}
%
% THE BIG TABLE: datasets and relevant papers, to be classified


